{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f3d9bd-b0a1-4e07-84ae-a368a8f65abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import trafilatura\n",
    "import json\n",
    "from typing import Optional, Dict, Any, List, Dict\n",
    "import logging\n",
    "import anthropic\n",
    "import os\n",
    "\n",
    "cryptonews_api = os.environ.get('CRYPTONEWS_API')\n",
    "claude_api = os.environ.get('CLAUDE_API')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6520ec93-597e-477e-a27f-38cf4ce5f907",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f\"https://cryptonews-api.com/api/v1?tickers=BTC&items=10&type=article&sortby=rank&days=3&page=1&token={cryptonews_api}\"\n",
    "response = requests.get(url)\n",
    "data = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0b94f7-8126-4d64-8538-7b57609b9db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticleExtractor:\n",
    "    \"\"\"A class to extract clean article content from various news websites.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        \n",
    "    def extract_content(self, url: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Extract article content from a given URL using multiple fallback methods.\n",
    "        \n",
    "        Args:\n",
    "            url: The URL of the news article\n",
    "            \n",
    "        Returns:\n",
    "            Dict containing extracted content with keys:\n",
    "            - title: Article title\n",
    "            - text: Main article text\n",
    "            - author: Author name (if available)\n",
    "            - date: Publication date (if available)\n",
    "            - summary: Article summary/description (if available)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # First try using trafilatura as it's typically most reliable\n",
    "            downloaded = trafilatura.fetch_url(url)\n",
    "            if downloaded:\n",
    "                result = trafilatura.extract(downloaded, include_comments=False, \n",
    "                                          include_tables=False, \n",
    "                                          output_format='json')\n",
    "                if result:\n",
    "                    content = json.loads(result)\n",
    "                    return {\n",
    "                        'title': content.get('title', ''),\n",
    "                        'text': content.get('text', ''),\n",
    "                        'author': content.get('author', ''),\n",
    "                        'date': content.get('date', ''),\n",
    "                        'summary': content.get('description', '')\n",
    "                    }\n",
    "            \n",
    "            # Fallback to BeautifulSoup method\n",
    "            return self._extract_with_beautifulsoup(url)\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error extracting content from {url}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def _extract_with_beautifulsoup(self, url: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Fallback method using BeautifulSoup with common article patterns.\"\"\"\n",
    "        try:\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "            }\n",
    "            response = requests.get(url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Remove unwanted elements\n",
    "            for element in soup.find_all(['script', 'style', 'nav', 'header', 'footer', 'iframe']):\n",
    "                element.decompose()\n",
    "            \n",
    "            # Extract title (try multiple common patterns)\n",
    "            title = ''\n",
    "            title_candidates = [\n",
    "                soup.find('meta', property='og:title'),\n",
    "                soup.find('meta', property='twitter:title'),\n",
    "                soup.find('h1'),\n",
    "                soup.find(class_=['article-title', 'entry-title', 'post-title'])\n",
    "            ]\n",
    "            for candidate in title_candidates:\n",
    "                if candidate:\n",
    "                    title = candidate.get('content', candidate.text.strip())\n",
    "                    if title:\n",
    "                        break\n",
    "            \n",
    "            # Extract main content (try multiple common patterns)\n",
    "            content = ''\n",
    "            content_candidates = [\n",
    "                soup.find(class_=['article-content', 'entry-content', 'post-content', 'main-content']),\n",
    "                soup.find('article'),\n",
    "                soup.find(role='main'),\n",
    "                soup.find(id=['content', 'main-content', 'article-content'])\n",
    "            ]\n",
    "            \n",
    "            for candidate in content_candidates:\n",
    "                if candidate:\n",
    "                    content = ' '.join(p.text.strip() for p in candidate.find_all('p') if p.text.strip())\n",
    "                    if content:\n",
    "                        break\n",
    "            \n",
    "            # Extract metadata\n",
    "            author = ''\n",
    "            author_candidates = [\n",
    "                soup.find('meta', property='author'),\n",
    "                soup.find('meta', name='author'),\n",
    "                soup.find(class_=['author', 'article-author', 'entry-author'])\n",
    "            ]\n",
    "            for candidate in author_candidates:\n",
    "                if candidate:\n",
    "                    author = candidate.get('content', candidate.text.strip())\n",
    "                    if author:\n",
    "                        break\n",
    "            \n",
    "            # Extract date\n",
    "            date = ''\n",
    "            date_candidates = [\n",
    "                soup.find('meta', property='article:published_time'),\n",
    "                soup.find('meta', property='article:modified_time'),\n",
    "                soup.find(class_=['date', 'article-date', 'entry-date'])\n",
    "            ]\n",
    "            for candidate in date_candidates:\n",
    "                if candidate:\n",
    "                    date = candidate.get('content', candidate.text.strip())\n",
    "                    if date:\n",
    "                        break\n",
    "                        \n",
    "            # Extract summary/description\n",
    "            summary = ''\n",
    "            summary_candidates = [\n",
    "                soup.find('meta', property='og:description'),\n",
    "                soup.find('meta', name='description'),\n",
    "                soup.find(class_=['article-summary', 'entry-summary'])\n",
    "            ]\n",
    "            for candidate in summary_candidates:\n",
    "                if candidate:\n",
    "                    summary = candidate.get('content', candidate.text.strip())\n",
    "                    if summary:\n",
    "                        break\n",
    "            \n",
    "            return {\n",
    "                'title': title,\n",
    "                'text': content,\n",
    "                'author': author,\n",
    "                'date': date,\n",
    "                'summary': summary\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in BeautifulSoup extraction for {url}: {str(e)}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b318bef4-346b-4705-b115-c1c1c254b6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = ArticleExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeb2f20-7512-4017-b32e-a20c3e93375a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in data['data']:\n",
    "    content = extractor.extract_content(d['news_url'])\n",
    "    if content is None:\n",
    "        d['main_content'] = ''\n",
    "    else:\n",
    "        d['main_content'] = content['text']\n",
    "\n",
    "articles = data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941da8cd-b16c-42ed-b34a-9e4e2f823a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea60455b-b090-4cff-87b8-608d12c07e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_bitcoin_news(api_key: str, \n",
    "                        news_articles: List[Dict],\n",
    "                        system_prompt: str = \"\"\"You are an AI journalist specializing in cryptocurrency news analysis. \n",
    "                        Analyze multiple Bitcoin news articles and create a comprehensive summary of the most important developments and trends.\n",
    "                        Always cite your claims using the provided URLs.\"\"\") -> str:\n",
    "    \"\"\"\n",
    "    Analyze multiple Bitcoin news articles and generate a comprehensive summary with citations\n",
    "    \n",
    "    Parameters:\n",
    "    api_key (str): Your Anthropic API key\n",
    "    news_articles (List[Dict]): List of dictionaries containing news articles\n",
    "    system_prompt (str): System prompt for Claude\n",
    "    \n",
    "    Returns:\n",
    "    str: Comprehensive analysis of all articles\n",
    "    \"\"\"\n",
    "    \n",
    "    client = anthropic.Client(api_key=api_key)\n",
    "    \n",
    "    # Format all articles into a single text\n",
    "    formatted_articles = \"\\n\\n===ARTICLE===\\n\\n\".join([\n",
    "        f\"\"\"Title: {article['title']}\n",
    "        Source: {article['source_name']}\n",
    "        Date: {article['date']}\n",
    "        URL: {article['news_url']}\n",
    "        \n",
    "        Content:\n",
    "        {article['main_content']}\"\"\"\n",
    "        for article in news_articles\n",
    "    ])\n",
    "    \n",
    "    user_prompt = \"\"\"Please analyze these Bitcoin news articles and create a comprehensive summary. Focus on:\n",
    "    1. Major price movements and market trends\n",
    "    2. Significant regulatory developments\n",
    "    3. Notable institutional or corporate developments\n",
    "    4. Key market sentiment indicators\n",
    "    \n",
    "    Requirements:\n",
    "    - Include a reference list with the URLs at the end. This is very important.\n",
    "    - Write in a direct journalistic style\n",
    "    - Organize the information thematically\n",
    "    - Highlight any emerging trends or patterns across multiple articles\n",
    "    - Note any conflicting information or perspectives\n",
    "    - Conclude with the most important takeaways\n",
    "    \n",
    "    Here are the articles to analyze:\n",
    "    \n",
    "    \"\"\" + formatted_articles\n",
    "    \n",
    "    try:\n",
    "        # Make API call to Claude\n",
    "        message = client.messages.create(\n",
    "            model=\"claude-3-opus-20240229\",\n",
    "            max_tokens=4096,  # Increased token limit for comprehensive analysis\n",
    "            system=system_prompt,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": user_prompt\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        return message.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing articles: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace355c4-0447-4471-ab95-cf5db2ff2b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the articles\n",
    "result = analyze_bitcoin_news(claude_api, articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e064d97f-8920-4756-8be8-c21ac61c9fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result[0].model_dump()['text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
